{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO74s/XFqWmomPzE/x//59k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gh5al/Statiscal-and-Mathematical-methods-for-AI/blob/main/LAB/Homework3/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ucoD-p2XRV"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backtracking(f, grad_f, x):\n",
        "    \"\"\"\n",
        "    This function is a simple implementation of the backtracking algorithm for\n",
        "    the GD (Gradient Descent) method.\n",
        "    \n",
        "    f: function. The function that we want to optimize.\n",
        "    grad_f: function. The gradient of f(x).\n",
        "    x: ndarray. The actual iterate x_k.\n",
        "    \"\"\"\n",
        "    alpha = 1\n",
        "    c = 0.8\n",
        "    tau = 0.25\n",
        "    \n",
        "    while f(x - alpha * grad_f(x)) > f(x) - c * alpha * np.linalg.norm(grad_f(x), 2) ** 2:\n",
        "        alpha = tau * alpha\n",
        "        \n",
        "        if alpha < 1e-3:\n",
        "            break\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "L3keNy0i6EkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRADIENT DESCENT"
      ],
      "metadata": {
        "id": "36vBwijE5_ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The gradient_descent implementation.\n",
        "# We don't know the final length of the arrays because the gradient can \n",
        "# converge early than expected. So we put a limit on the maxium number of iteration\n",
        "def gradient_descent(f, grad_f, x0, kmax, tolf, tolx):\n",
        "    # Initialization\n",
        "    k = 0\n",
        "    n = int(x0.shape[0])\n",
        "    x = np.zeros((kmax + 1, n))\n",
        "    f_val = np.zeros((kmax + 1, ))\n",
        "    grads = np.zeros((kmax + 1, n))\n",
        "    err = np.zeros((kmax + 1, ))\n",
        "    \n",
        "    # Assign the values for the first iteration, start point(k=0)\n",
        "    x[k, :] = x0\n",
        "    f_val[k] = f(x[k, :])\n",
        "    grads[k, :] = grad_f(x[k, :])\n",
        "    # the err is a vector, because it's the norm of the gradient   \n",
        "    err[k] = np.linalg.norm(grads[k, :])\n",
        "    \n",
        "    # Choose step size\n",
        "    alpha = 0.1\n",
        "    \n",
        "    # Handle the condition for the first iteration (x[k,:] - x[k-1,:])\n",
        "    \n",
        "    if k == 0:\n",
        "        x[-1, :] = np.ones((n,))\n",
        "        x[k-1, :] = x[-1, :]\n",
        "        \n",
        "    \n",
        "    # Start the iterations\n",
        "    while ((k < kmax) and (err[k] >= (tolf * err[0])) and (np.linalg.norm(x[k,:] - x[k-1,:]) >= tolx)):\n",
        "        # Update the value of x\n",
        "        x[k+1, :] = x[k, :] - alpha * grads[k, :]\n",
        "        \n",
        "        # Update the step size alpha with backtracking, it computes a good alpha which ensures the convergence of the gradient\n",
        "        alpha = backtracking(f, grad_f, x[k, :])\n",
        "        \n",
        "        # Update the values the the actual iteration\n",
        "        k = k+1\n",
        "        f_val[k] = f(x[k, :]) \n",
        "        grads[k, :] = grad_f(x[k, :])\n",
        "        err[k] = np.linalg.norm(grads[k, :])\n",
        "    \n",
        "    # Truncate the vectors that are (eventually) too long\n",
        "    x = x[:k+1, :] \n",
        "    f_val = f_val[:k+1]\n",
        "    grads = grads[: k+1]\n",
        "    err = err[: k+1]\n",
        "    \n",
        "    return x, k, f_val, grads, err"
      ],
      "metadata": {
        "id": "Mg-lmqOo5nfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 1:\n",
        "def f(x):\n",
        "    return ((x[0]-3)**2 + (x[1] - 1)**2)\n",
        "\n",
        "\n",
        "def grad_f(x):\n",
        "   # gf = np.empty_like(x)\n",
        "    #gf[0] = (2*(x[0]- 1))\n",
        "    #gf[1] = (2*(x[1]-1))\n",
        "    return np.array([2*(x[0]- 3), 2*(x[1]-1)])\n",
        "\n",
        "n = 2\n",
        "\n",
        "x0 = np.zeros((n, ))\n",
        "\n",
        "kmax = 100\n",
        "tolf = 1e-6\n",
        "tolx = 1e-5\n",
        "x_truth = [3,1]\n",
        "\n",
        "x, k, f_val, grads, err = gradient_descent(f, grad_f, x0, kmax, tolf, tolx)\n",
        "\n",
        "plt.plot(err)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('error')\n",
        "plt.show()\n",
        "\n",
        "t_err = []\n",
        "for a in x:\n",
        "    t_err.append(np.linalg.norm(a-x_truth))\n",
        "    print(f\"x: {a}, truth: {x_truth}\")\n",
        "\n",
        "plt.plot(t_err)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('truth_error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h_JtPhdZ5sPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ax = np.linspace(-5, 4, 100) # [-5, 4] = [a,b] interval which will contain 100 points\n",
        "y_ax = np.linspace(-6, 6, 100) # [-6, 6] = [c,d]\n",
        "xv, yv = np.meshgrid(x_ax, y_ax) #corresponds to the points in the grid\n",
        "z_ax = f([xv,yv]) \n",
        "\n",
        "contours = plt.contour(x_ax, y_ax, z_ax)\n",
        "plt.plot(x[:,0], x[: , 1], '-o' ) #used to show the path of the gradient \n",
        "plt.show()\n",
        "#show plot with different values of alpha like, 1(diverges), 1e-4( not moving enough) and also the behaviour \n",
        "#with backtracking, backtracking chooses the right step-size to go to the minimum smoothly without bouncing too much\n"
      ],
      "metadata": {
        "id": "Roa881cN5vnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 2:\n",
        "def f(x):\n",
        "    return (10*(x[0]-1)**2 + (x[1]-2)**2)\n",
        "\n",
        "\n",
        "def grad_f(x):\n",
        "   # gf = np.empty_like(x)\n",
        "    #gf[0] = (2*(x[0]- 1))\n",
        "    #gf[1] = (2*(x[1]-1))\n",
        "    return np.array([20*(x[0]- 1), 2*(x[1]-2)])\n",
        "\n",
        "n = 2\n",
        "\n",
        "x0 = np.zeros((n, ))\n",
        "\n",
        "kmax = 100\n",
        "tolf = 1e-6\n",
        "tolx = 1e-5\n",
        "\n",
        "x, k, f_val, grads, err = gradient_descent(f, grad_f, x0, kmax, tolf, tolx)"
      ],
      "metadata": {
        "id": "5BM8Lir_50KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "v = np.linspace(0, 1, n)\n",
        "print(v)\n",
        "A = np.vander(v, n)\n",
        "x_t = np.ones(n)\n",
        "b = A @ x_t\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    return 0.5 * np.linalg.norm(A@x - b, 2) ** 2\n",
        "\n",
        "\n",
        "def grad_f(x):\n",
        "    return A.T@(A@x-b) \n",
        "\n",
        "\n",
        "x0 = np.zeros((n, ))\n",
        "\n",
        "kmax = 100\n",
        "tolf = 1e-6\n",
        "tolx = 1e-5\n",
        "\n",
        "x, k, f_val, grads, err = gradient_descent(f, grad_f, x0, kmax, tolf, tolx)\n",
        "\n",
        "plt.plot(err)\n",
        "plt.title('function_3')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wt-zYNQT542D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "v = np.linspace(0, 1, n)\n",
        "print(v)\n",
        "A = np.vander(v, n)\n",
        "x_t = np.ones(n)\n",
        "b = A @ x_t\n",
        "l = 0.3 #lamda between [0,1]\n",
        "\n",
        "def f(x):\n",
        "    return 0.5 * np.linalg.norm(A@x - b, 2) ** 2 + l * 0.5 * np.linalg.norm(x)**2\n",
        "\n",
        "\n",
        "def grad_f(x):\n",
        "    return A.T@(A@x-b) + x\n",
        "\n",
        "\n",
        "x0 = np.zeros((n, ))\n",
        "\n",
        "kmax = 100\n",
        "tolf = 1e-6\n",
        "tolx = 1e-5\n",
        "\n",
        "x, k, f_val, grads, err = gradient_descent(f, grad_f, x0, kmax, tolf, tolx)\n",
        "\n",
        "plt.plot(err)\n",
        "plt.title('function_4')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5-MwTaSd57Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SGD"
      ],
      "metadata": {
        "id": "CamTWd3V58of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(f, grad_f, w0, data, batch_size, n_epochs):\n",
        "    # Extract data\n",
        "    x, y = data\n",
        "    print(x)\n",
        "    # Initialize\n",
        "    w_val = np.zeros((n_epochs + 1, int(w0.shape[0])))\n",
        "    f_val = np.zeros((n_epochs + 1,))\n",
        "    grads = np.zeros((n_epochs + 1, int(w0.shape[0]) ))\n",
        "    err = np.zeros((n_epochs + 1,))\n",
        "    \n",
        "    # Assign values for the first iteration\n",
        "    w_val[0, :] = w0\n",
        "    f_val[0] = f(w0, x, y)\n",
        "    grads[0, :] = grad_f(w0, x, y) \n",
        "    err[0] = np.linalg.norm(grads[0, :])\n",
        "    \n",
        "    # Choose step size\n",
        "    alpha = 0.1\n",
        "    \n",
        "    w = w0\n",
        "    # Copy the data\n",
        "    x_copy = np.copy(x)\n",
        "    y_copy = np.copy(y)\n",
        "    # Compute the number of batch iteration for each epoch\n",
        "    N = np.size(x, axis = 1)\n",
        "    n_iter_per_epoch = int(N / batch_size)\n",
        "    # For each epoch\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        \n",
        "        # Inner iterations\n",
        "        for k in range(n_iter_per_epoch):\n",
        "            # Random indices that composes our mini-batch (look at np.random.choice)\n",
        "            batch_idx = np.random.choice(x.shape[1], batch_size, replace = False )\n",
        "           # print(batch_idx)\n",
        "            # Split\n",
        "            mask = np.ones((x.shape[1], ), dtype=bool)\n",
        "          #  print(x.shape[1])\n",
        "            mask[batch_idx] = False\n",
        "            \n",
        "            x_batch = x[:, ~mask]\n",
        "            y_batch = y[~mask]\n",
        "            \n",
        "            x = x[:, mask]\n",
        "            y = y[mask]\n",
        "            \n",
        "            # Update weights\n",
        "            w = w - alpha * grad_f(w, x_batch, y_batch)\n",
        "            \n",
        "        # Refill the data\n",
        "        x = np.copy(x_copy)\n",
        "        y = np.copy(y_copy)\n",
        "        \n",
        "        # Update the values of the vector after each epoch\n",
        "        w_val[epoch] = w\n",
        "        f_val[epoch] = f(w_val[epoch, :], x, y)\n",
        "        grads[epoch, :] = grad_f(w_val[epoch, :], x, y)\n",
        "        err[epoch] = np.linalg.norm(grads[epoch, :], 2)\n",
        "    \n",
        "    # Truncate the excess\n",
        "    w_val  = w_val[:epoch, :]\n",
        "    f_val = f_val[:epoch]\n",
        "    grads = grads[:epoch, :]\n",
        "    err = err[:epoch]\n",
        "    \n",
        "    return w_val, f_val, grads, err"
      ],
      "metadata": {
        "id": "H5Fd_cDf6DCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "N = 100\n",
        "\n",
        "w_t = np.ones((n))\n",
        "x = np.random.rand(n, N) \n",
        "\n",
        "#print(x)\n",
        "\n",
        "sigma = 0.1\n",
        "shape = N\n",
        "eta = np.random.normal(0, sigma, shape)\n",
        "\n",
        "y = np.array([w_t @ x[:, k] for k in range(N) ]) \n",
        "y_tilde = np.array([(w_t @ x[:, k] + np.random.normal(0, 0.1)) for k in range(N)])\n",
        "z = np.array([w_t @ (np.square(x[:, k])) for k in range(N)])\n",
        "\n",
        "#print(y)\n",
        "#print(y_tilde)\n",
        "#print(z)\n",
        "\n",
        "#def f2(w, x, y):\n",
        " #   a = np.array([(x[:, k] @ w - y[k])**2 for k in range(y.shape[0])])\n",
        "  #  s = np.sum(a)\n",
        "   # return s \n",
        "\n",
        "\n",
        "def f(w, x, y):\n",
        "    return 0.5 * np.linalg.norm(x.T@w - y, 2) ** 2\n",
        "\n",
        "#print(f(w_t, x, y_tilde))\n",
        "#print(f2(w_t, x, y_tilde))\n",
        "\n",
        "#print(x.T.shape)\n",
        "#print(w.shape)\n",
        "#print(y.shape)\n",
        "\n",
        "\n",
        "def grad_f(w, x, y):\n",
        "   return  x @(x.T@w - y) \n",
        "\n",
        "#print(grad_f(w_t, x , z))\n",
        "#print(grad_f2(w_t, x , z))\n"
      ],
      "metadata": {
        "id": "fNU96N_K6eaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = np.zeros((n,))\n",
        "data = [x, y_tilde]\n",
        "batch_size = 5\n",
        "n_epochs = 10\n",
        "\n",
        "w_approx, f_val, grads, err = SGD(f, grad_f, w0, data, batch_size, n_epochs)\n",
        "\n",
        "\n",
        "plt.plot(err)\n",
        "plt.title('square loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('error')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Plot the error for y, y_tilde and z\n",
        "# Try also different epochs and batch size, also different N and n"
      ],
      "metadata": {
        "id": "gV23rt7k6fLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}